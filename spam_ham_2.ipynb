{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\", \n",
    "        color_codes = True,\n",
    "        font_scale = 1.5)\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('spam_ham_data.zip') as item:\n",
    "    item.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_training_data = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Convert the emails to lowercase as the first step of text processing.\n",
    "original_training_data['email'] = original_training_data['email'].str.lower()\n",
    "test['email'] = test['email'].str.lower()\n",
    "\n",
    "original_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing or NAN values.\n",
    "print('Before imputation:')\n",
    "print(original_training_data.isnull().sum())\n",
    "original_training_data = original_training_data.fillna('')\n",
    "print('------------')\n",
    "print('After imputation:')\n",
    "print(original_training_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a 90/10 train-validation split on our labeled data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(original_training_data, test_size = 0.1, random_state = 42)\n",
    "\n",
    "# We must do this in order to preserve the ordering of emails to labels for words_in_texts.\n",
    "train = train.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projB2_utils import words_in_texts\n",
    "\n",
    "words_in_texts(['hello', 'bye', 'world'], pd.Series(['hello', 'hello worldhello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_words = ['drug', 'bank', 'prescription', 'memo', 'private']\n",
    "\n",
    "X_train = words_in_texts(some_words, train['email'])\n",
    "Y_train = np.array(train['spam'])\n",
    "\n",
    "X_train[:5], Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "simple_model = LogisticRegression()\n",
    "simple_model.fit(X_train, Y_train)\n",
    "\n",
    "training_accuracy = simple_model.score(X_train, Y_train)\n",
    "print(\"Training Accuracy: \", training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "train_graph = train.copy()\n",
    "train_graph['word_count'] = train_graph['email'].apply(lambda text: len(text.split()))\n",
    "\n",
    "# Create a box plot to compare word count in spam and ham emails\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='spam', y='word_count', data=train_graph)#, palette={'spam': 'red', 'ham': 'blue'})\n",
    "plt.title('Word Count Distribution in Spam and Ham Emails')\n",
    "plt.xlabel('Ham = 0, Spam = 1')\n",
    "plt.ylabel('Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The violin plot above shows the distribution of the average word count of emails classified as ham and spam within the training set. We can see that both ham and spam emails have similar median values but ham values have a greater maximum value. There also seems to be a slightly greater spread for spam emails while ham emails are more dense around the 150 mark.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# You may use any of these to create your features.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"free\", \"for you\", \"win\", \"increase\", \"special\", \"click\", \"guarantee\", \n",
    "         \"html\", \"please\", \"body\", \"name\", \"opportunity\", \"sir\", \"signup\"]\n",
    "\n",
    "X = pd.DataFrame(words_in_texts(words, train[\"email\"]))\n",
    "Y = train[\"spam\"]\n",
    "\n",
    "for i in range(len(words)):\n",
    "    X.rename(columns={X.columns[i]: words[i]}, inplace=True)\n",
    "\n",
    "X[\"Email Length\"] = (train[\"email\"].str.len() < 120) == (train['email'].str.len() > 27000).astype(int)\n",
    "X[\"Subject Length\"] = (train[\"subject\"].str.len() < 20) == (train[\"subject\"].str.len() > 70).astype(int)\n",
    "\n",
    "X_train = np.array(X)\n",
    "Y_train = np.array(Y)\n",
    "\n",
    "model = LogisticRegression(fit_intercept = True, penalty = \"l2\")\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(X_train)\n",
    "\n",
    "# Calculate the accuracy of the model on the testing set\n",
    "training_accuracy = np.mean(train_predictions == train[\"spam\"])\n",
    "\n",
    "# Print your testing accuracy\n",
    "training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = pd.DataFrame(words_in_texts(words, test[\"email\"]))\n",
    "\n",
    "for i in range(len(words)):\n",
    "    X_t.rename(columns={X_t.columns[i]: words[i]}, inplace=True)\n",
    "\n",
    "X_t[\"Email Length\"] = (test[\"email\"].str.len() < 120) == (test['email'].str.len() > 30000).astype(int)\n",
    "X_t[\"Subject Length\"] = (test[\"subject\"].str.len() < 19) == (test[\"subject\"].str.len() > 50).astype(int)\n",
    "\n",
    "X_test = np.array(X_t)\n",
    "\n",
    "test_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set are stored in a 1-dimensional array called test_predictions.\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"Id\": test['id'], \n",
    "    \"Class\": test_predictions,\n",
    "}, columns=['Id', 'Class'])\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = \"submission_{}.csv\".format(timestamp)\n",
    "submission_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''I found out that the chosen words affected my model's performance the greatest, I sought out to implement the graph we implemented in Q3 of Proj B1 to find good candidates that can indicate spam emails from ham emails. I also had the idea to integrate the email and subject word counts while working on my model, I found out later on that simply using the numerical value was not gonna work and it would be more feasible to sort of quantify the numerical values to boolean values. I was surprised that certain words I thought would be indicative of spam emails turned our to appear just as frequently in ham emails causing me to be more careful in selecting features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_prob = model.predict_proba(X_train)[:, 1]\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(Y_train, Y_prob)\n",
    "\n",
    "plt.plot(thresholds, tpr, label=\"True Positive Rate\")\n",
    "plt.plot(thresholds, fpr, label=\"False Positive Rate\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"TPR/FPR\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
